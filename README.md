# Smart Summary App

A full-stack application that allows users to paste a block of text and receive a summary generated by a Large Language Model (LLM).

## Project Structure

This is a monorepo containing:

- `apps/frontend`: Next.js frontend application with TypeScript and Tailwind CSS
- `apps/backend`: FastAPI backend service with Python

## Features

- Text summarization using LLM technology
- Real-time streaming of summary results
- Clean, responsive UI
- Error handling and validation
- Character and word count statistics

## Tech Stack

### Frontend

- Next.js with TypeScript
- Tailwind CSS for styling
- React hooks for state management
- Fetch API for data fetching and streaming

### Backend

- FastAPI (Python)
- OpenAI API integration (configurable for other LLM providers)
- Server-sent events for streaming responses
- Environment-based configuration

## Setup Instructions

### Prerequisites

- Node.js (v16+)
- Python (v3.8+)
- npm or yarn
- OpenAI API key (or another LLM provider API key)

### Quick Setup

For a quick setup of both frontend and backend:

```bash
# Clone the repository
git clone https://github.com/yourusername/smart-summary-app.git
cd smart-summary-app

# Install all dependencies (frontend and root)
npm run install:all

# Set up backend
npm run setup:backend

# Configure your OpenAI API key
# Edit apps/backend/.env and add your API key
# OPENAI_API_KEY=your_api_key_here

# Start both services
npm run dev
```

### Frontend Setup

```bash
# Navigate to the frontend directory
cd apps/frontend

# Install dependencies
npm install

# Create a .env.local file with the API URL
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > .env.local

# Start the development server
npm run dev
```

The frontend will be available at http://localhost:3000

### Backend Setup

```bash
# Navigate to the backend directory
cd apps/backend

# Create a virtual environment (optional but recommended)
python -m venv venv

# Activate the virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
# source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create a .env file with your API keys
# Copy the example file and edit it
cp .env.example .env
# Edit the .env file to add your OpenAI API key

# Start the development server
python run.py
```

The API will be available at http://localhost:8000

### Running Both Services

From the project root, after installing dependencies:

```bash
# Run both services
npm run dev
```

## Testing the Application

Once both the frontend and backend are running, you can test the application:

1. Open your browser and navigate to http://localhost:3000
2. Click on "Get Started" or navigate to http://localhost:3000/summarize
3. Paste a block of text in the input field
4. Click "Summarize (Stream)" to see the summary generated in real-time or "Summarize (Regular)" for a standard request

### Sample Text for Testing

Here's a sample text you can use to test the application:

```
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.

The term "artificial intelligence" had previously been used to describe machines that mimic and display "human" cognitive skills that are associated with the human mind, such as "learning" and "problem-solving". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.

AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).

As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.
```

## API Endpoints

### GET /

- Returns a simple message indicating the API is running

### GET /health

- Health check endpoint

### POST /summarize

- Accepts a JSON body with `text`, `max_length` (optional), and `model` (optional)
- Returns a summary of the provided text

### POST /summarize/stream

- Accepts the same parameters as `/summarize`
- Returns a server-sent event stream with the summary tokens

## Architecture

The application follows a client-server architecture:

1. **Frontend (Next.js)**

   - User interface for text input and summary display
   - Communicates with backend API
   - Handles both regular and streaming responses

2. **Backend (FastAPI)**

   - Receives requests from frontend
   - Processes text using LLM service
   - Returns responses (regular or streaming)

3. **LLM Integration**
   - Abstracts the LLM provider behind a service interface
   - Currently uses OpenAI but can be extended to other providers

## Future Improvements

- User authentication and saved summaries
- Multiple LLM provider options
- Customization options (summary length, style, etc.)
- Caching for repeated requests
- Analytics for usage tracking
- Mobile app version

## Scaling and Security Considerations

### Scaling

- Horizontal scaling for increased load
- Caching layer for repeated requests
- Rate limiting for API endpoints
- Asynchronous processing for long texts

### Security

- Input validation and sanitization
- API key rotation
- Environment-based configuration
- CORS configuration for production
- Rate limiting to prevent abuse
- Proper error handling to avoid information leakage

## Quick Start with Docker

### Prerequisites

- Docker and Docker Compose installed
- OpenAI API key

### Setup

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd smart-summary-app
   ```

2. **Set up environment variables**

   ```bash
   # Create .env file
   echo "OPENAI_API_KEY=your_openai_api_key_here" > .env
   ```

3. **Run with Docker Compose**

   ```bash
   docker-compose up --build
   ```

4. **Access the application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000

### Docker Commands

```bash
# Build and start all services
docker-compose up --build

# Run in background
docker-compose up -d

# Stop all services
docker-compose down

# View logs
docker-compose logs

# Rebuild specific service
docker-compose build backend
docker-compose build frontend
```

## Development Setup

### Prerequisites

- Node.js 22+
- Python 3.12+
- OpenAI API key

### Local Development

1. **Install dependencies**

   ```bash
   npm install
   ```

2. **Set up environment**

   ```bash
   echo "OPENAI_API_KEY=your_openai_api_key_here" > .env
   ```

3. **Start development servers**

   ```bash
   npm run dev
   ```

4. **Access the application**
   - Frontend: http://localhost:3000
   - Backend: http://localhost:8000

## Project Structure

```
smart-summary-app/
├── apps/
│   ├── backend/          # FastAPI backend
│   │   ├── app/
│   │   │   ├── main.py           # API endpoints
│   │   │   ├── models.py         # Data models
│   │   │   └── services/
│   │   │       └── llm_service.py # OpenAI integration
│   │   ├── Dockerfile
│   │   └── requirements.txt
│   └── frontend/         # Next.js frontend
│       ├── src/
│       │   └── app/
│       │       └── page.tsx      # Main application
│       ├── Dockerfile
│       └── package.json
├── docker-compose.yml
└── package.json
```

## API Endpoints

- `GET /health` - Health check
- `GET /example` - Get example text
- `POST /summarize` - Summarize text

## Environment Variables

- `OPENAI_API_KEY` - Your OpenAI API key (required)
- `NEXT_PUBLIC_API_URL` - Backend API URL (default: http://localhost:8000)

## Technology Stack

- **Frontend**: Next.js 15, React, TypeScript, Tailwind CSS
- **Backend**: FastAPI, Python 3.12, OpenAI API
- **Deployment**: Docker, Docker Compose

## Features

### Smart Caching

- Caches up to 50 previous summaries
- Instant results for repeated text
- Automatic cleanup of old entries

### User Experience

- Paste detection anywhere on the page
- Loading states with animated spinner
- Copy to clipboard functionality
- Responsive design for all devices

## Deployment

### AWS Deployment Options

1. **AWS Lightsail** (Recommended for simple deployments)

   - Create a Lightsail instance
   - Install Docker and Docker Compose
   - Clone repository and run `docker-compose up -d`

2. **AWS ECS Fargate**

   - Push images to ECR
   - Create ECS task definitions
   - Deploy with Application Load Balancer

3. **AWS EC2**
   - Launch EC2 instance
   - Install Docker and Docker Compose
   - Clone repository and run `docker-compose up -d`

### Production Considerations

- Set up proper SSL certificates
- Configure environment variables securely
- Set up monitoring and logging
- Use a reverse proxy (nginx) for production
- Configure proper CORS settings

## License

MIT License
